{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c63e3b5"
      },
      "source": [
        "### IMPORTANDO AS BILIOTECAS NECESSÁRIAS"
      ],
      "id": "9c63e3b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8svNejARnN8-",
        "outputId": "ff6c0df1-4d28-4a77-88c4-0a5d2d2bf00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.16.0+cu118)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: timm==0.9.2 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install segmentation_models_pytorch"
      ],
      "id": "8svNejARnN8-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZyaSYxOP1iD",
        "outputId": "d77544e7-714e-4c7b-eeec-cb6439d05fe4"
      },
      "id": "rZyaSYxOP1iD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c11eb34"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#Controlando o comportamento da biblioteca CUDA em Python\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import   numpy   as   np\n",
        "import   cv2\n",
        "import   matplotlib.pyplot   as   plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "import segmentation_models_pytorch as smp\n",
        "import segmentation_models_pytorch.utils.metrics"
      ],
      "id": "0c11eb34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZLNtLsF-X1y"
      },
      "outputs": [],
      "source": [],
      "id": "7ZLNtLsF-X1y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90b1074"
      },
      "source": [
        "### CHECANDO A UTILIZAÇÃO DA PLACA DE VÍDEO"
      ],
      "id": "a90b1074"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a262ee76",
        "outputId": "a5aa08be-feca-4277-cb42-f5fa82f4d913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('\\nDevice: {0}'.format(DEVICE))"
      ],
      "id": "a262ee76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfc8e13d"
      },
      "source": [
        "### LIMITANDO A GERAÇÃO DE NÚMEROS ALEATÓRIOS NOS TESTES"
      ],
      "id": "bfc8e13d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcd65090"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "id": "fcd65090"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c47a5c56"
      },
      "source": [
        "### DEFININDO OS DIRETÓRIOS DO DATASET"
      ],
      "id": "c47a5c56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c291c70",
        "outputId": "e6a2b119-e7d3-457e-f8f8-3817af4e287a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ],
      "source": [
        "#DATA_DIR = '/home/jocsan/SegNet-Tutorial/CamVid'\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "\n",
        "DATA_DIR = (\"/gdrive/MyDrive/CamVid/class_dict.csv\")\n",
        "x_train_dir = '/content/gdrive/MyDrive/CamVid/train2'\n",
        "y_train_dir = '/content/gdrive/MyDrive/CamVid/train_labels'\n"
      ],
      "id": "1c291c70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e45e284"
      },
      "outputs": [],
      "source": [
        "x_train_dir = os.path.join(DATA_DIR, 'train2')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'testannot')"
      ],
      "id": "9e45e284"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8abd612"
      },
      "source": [
        "### DEFININDO A FUNÇÃO AUXILIAR PARA VIZUALIZAÇÃO DE DADOS"
      ],
      "id": "e8abd612"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd6152e2"
      },
      "outputs": [],
      "source": [
        "#Função auxiliar para visualização de dados\n",
        "def visualize(**images):\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "id": "fd6152e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8887f765"
      },
      "source": [
        "### CRIANDO A CLASSE \"DATASET\" PARA ORGANIZAR OS DADOS"
      ],
      "id": "8887f765"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a678613a"
      },
      "outputs": [],
      "source": [
        "class Dataset(BaseDataset):\n",
        "\n",
        "    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement',\n",
        "               'tree', 'signsymbol', 'fence', 'car',\n",
        "               'pedestrian', 'bicyclist', 'unlabelled']\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            images_dir,\n",
        "            masks_dir,\n",
        "            classes=None,\n",
        "            augmentation=None, #Expansão de dados\n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "\n",
        "        ##Converte nomes de str em valores de classe em máscaras\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        ##Lendo dados\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.masks_fps[i], 0)\n",
        "\n",
        "        ##Extrai certas classes da máscara (por exemplo, carros)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "\n",
        "\n",
        "        #Aplicar aumentos\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "\n",
        "        #Aplicar pré-processamento\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n"
      ],
      "id": "a678613a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28184647"
      },
      "source": [
        "### VISUALIZANDO OS DADOS QUE TEMOS ATÉ O MOMENTO"
      ],
      "id": "28184647"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13f5ef6a"
      },
      "outputs": [],
      "source": [
        "CLASSES = ['sky', 'building', 'pole', 'road', 'pavement',\n",
        "               'tree', 'signsymbol', 'fence', 'car',\n",
        "               'pedestrian', 'bicyclist', 'unlabelled']"
      ],
      "id": "13f5ef6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73ebb7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "e87f89f7-54f3-4ff6-f4c8-c7e9de62c95b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a4d17102014a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mteste\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCLASSES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mteste\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2d65194daf08>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, images_dir, masks_dir, classes, augmentation, preprocessing)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     ):\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/MyDrive/CamVid/class_dict.csv/train2'"
          ]
        }
      ],
      "source": [
        "for teste in CLASSES:\n",
        "    i = 1\n",
        "    dataset = Dataset(x_train_dir, y_train_dir, classes=[teste])\n",
        "    image, mask = dataset[i]\n",
        "\n",
        "    linha = '-'*50\n",
        "    print(linha, teste, linha)\n",
        "\n",
        "    visualize(\n",
        "        Image=image,\n",
        "        Mask=mask.squeeze(),\n",
        "    )\n"
      ],
      "id": "73ebb7c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecbf2663"
      },
      "source": [
        "### AUMENTO DE DADOS COM A BIBLIOTECA \"ALBUMENTATIONS\""
      ],
      "id": "ecbf2663"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55056bc2"
      },
      "outputs": [],
      "source": [
        "import albumentations as albu"
      ],
      "id": "55056bc2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f507bb5d"
      },
      "outputs": [],
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
        "\n",
        "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
        "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
        "\n",
        "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
        "        albu.IAAPerspective(p=0.5),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.CLAHE(p=1),\n",
        "                albu.RandomBrightness(p=1),\n",
        "                albu.RandomGamma(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.IAASharpen(p=1),\n",
        "                albu.Blur(blur_limit=3, p=1),\n",
        "                albu.MotionBlur(blur_limit=3, p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.RandomContrast(p=1),\n",
        "                albu.HueSaturationValue(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"\"Adicione preenchimentos para tornar a forma da imagem divisível por 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.PadIfNeeded(384, 480)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construir transformação de pré-processamento\n",
        "\n",
        "    Argumentos:\n",
        "        preprocessing_fn (callbale): função de normalização de dados\n",
        "            (pode ser específico para cada rede neural pré-treinada)\n",
        "    Retornar:\n",
        "        transform: albumentations.Compose\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)"
      ],
      "id": "f507bb5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "927aa143"
      },
      "outputs": [],
      "source": [
        "#Visualizar imagens e máscaras aumentadas\n",
        "\n",
        "augmented_dataset = Dataset(\n",
        "    x_train_dir,\n",
        "    y_train_dir,\n",
        "    augmentation=get_training_augmentation(),\n",
        "    classes=['pedestrian'],\n",
        ")\n",
        "\n",
        "#Mesma imagem com diferentes transformações aleatórias\n",
        "for i in range(3):\n",
        "    image, mask = augmented_dataset[1]\n",
        "    visualize(image=image, mask=mask.squeeze(-1))"
      ],
      "id": "927aa143"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14dad31a"
      },
      "source": [
        "### Resolvendo o erro \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\""
      ],
      "id": "14dad31a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dd0b5f7"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import ssl\n",
        "\n",
        "def main():\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "    r = urllib.request.urlopen('http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth')\n",
        "    print(r.status)\n",
        "    print(r)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "id": "6dd0b5f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b549a89b"
      },
      "source": [
        "### CRIANDO O MODELO E TREINANDO"
      ],
      "id": "b549a89b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88ee9f26"
      },
      "outputs": [],
      "source": [
        "ENCODER = 'se_resnext50_32x4d'#Modelo de rede neural convolucional (CNN)\n",
        "ENCODER_WEIGHTS = 'imagenet'  #Parâmetro para especificar os pesos pré-treinados a serem usados no codificador\n",
        "CLASSES = ['car']             #Classe escolhida para ser segmentada na imagem\n",
        "ACTIVATION = 'sigmoid'        #Pode ser None para logits ou 'softmax2d' para segmentação multiclasse\n",
        "DEVICE = 'cuda'               #Dispositivo CUDA em utilização\n",
        "\n",
        "#Criar modelo de segmentação com codificador pré-treinado\n",
        "model = smp.FPN(\n",
        "    encoder_name=ENCODER,                    #Codificador, por exemplo mobilenet_v2 ou eficientenet-b7\n",
        "    encoder_weights=ENCODER_WEIGHTS,         #Pesos pré-treinados `imagenet` para inicialização do codificador\n",
        "    in_channels=3,                           #Canais de entrada do modelo (1 para imagens em escala de cinza, 3 para RGB, etc.)\n",
        "    classes=len(CLASSES),                    #Canais de saída do modelo (número de classes em seu conjunto de dados)\n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "#Obtendo a função de pré-processamento adequada para codificador\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "id": "88ee9f26"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30b85606"
      },
      "outputs": [],
      "source": [
        "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
        "\n",
        "preprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')"
      ],
      "id": "30b85606"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37293ff2"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(\n",
        "    x_train_dir,\n",
        "    y_train_dir,\n",
        "    augmentation=get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir,\n",
        "    y_valid_dir,\n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
      ],
      "id": "37293ff2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f2bd2f8"
      },
      "outputs": [],
      "source": [
        "loss = smp.utils.losses.DiceLoss()\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    dict(params=model.parameters(), lr=0.0001),\n",
        "])"
      ],
      "id": "5f2bd2f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d0fa0d1"
      },
      "outputs": [],
      "source": [
        "# criar corredores de época\n",
        "# é um loop simples de iteração sobre as amostras do carregador de dados\n",
        "\n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "id": "3d0fa0d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0594ca33"
      },
      "source": [
        "### CRIANDO O MODELO DE TREINO PARA N ÉPOCAS"
      ],
      "id": "0594ca33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d832aad1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "max_score = 0\n",
        "\n",
        "for i in range(0, 1):\n",
        "\n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "\n",
        "    #Salvar modelo, alterar lr, etc.\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, './best_model.pth')\n",
        "        print('Model saved!')\n",
        "\n",
        "    if i == 25:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "        print('Decrease decoder learning rate to 1e-5!')"
      ],
      "id": "d832aad1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b6d1e15"
      },
      "outputs": [],
      "source": [
        "iou_scores=[]\n",
        "iou_scores.append(valid_logs['iou_score'])\n",
        "accuracy = sum(iou_scores) / len(iou_scores)\n",
        "print(f'Acurácia média (iou_score): {accuracy}')"
      ],
      "id": "5b6d1e15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94447290"
      },
      "source": [
        "### CARREGANDO O MELHOR MODELO SALVO"
      ],
      "id": "94447290"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49d2aeb9"
      },
      "outputs": [],
      "source": [
        "best_model = torch.load('./best_model.pth')"
      ],
      "id": "49d2aeb9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6208144f"
      },
      "source": [
        "### CRIANDO O TESTE E AVALIANDO O MODELO"
      ],
      "id": "6208144f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51d9bb81"
      },
      "outputs": [],
      "source": [
        "test_dataset = Dataset(\n",
        "    x_test_dir,\n",
        "    y_test_dir,\n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)"
      ],
      "id": "51d9bb81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fab541f7"
      },
      "outputs": [],
      "source": [
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model=best_model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "logs = test_epoch.run(test_dataloader)"
      ],
      "id": "fab541f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653d2230"
      },
      "source": [
        "### CONJUNTO DE DADOS DE TESTE SEM TRANSFORMAÇÕES, PARA VISUALIZAÇÃO DAS IMAGENS"
      ],
      "id": "653d2230"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84181ea2"
      },
      "outputs": [],
      "source": [
        "test_dataset_vis = Dataset(\n",
        "    x_test_dir, y_test_dir,\n",
        "    classes=CLASSES,\n",
        ")"
      ],
      "id": "84181ea2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80cd6f69"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    n = np.random.choice(len(test_dataset))\n",
        "\n",
        "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[n]\n",
        "\n",
        "    gt_mask = gt_mask.squeeze()\n",
        "\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    pr_mask = best_model.predict(x_tensor)\n",
        "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
        "\n",
        "    visualize(\n",
        "        image=image_vis,\n",
        "        ground_truth_mask=gt_mask,\n",
        "        predicted_mask=pr_mask\n",
        "    )"
      ],
      "id": "80cd6f69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "832758e6"
      },
      "outputs": [],
      "source": [],
      "id": "832758e6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}